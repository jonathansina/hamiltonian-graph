{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>hamiltonian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>graph4443.png</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>graph1905.png</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>graph7719.png</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>graph4902.png</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>graph4114.png</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4423</th>\n",
       "      <td>graph4672.png</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4424</th>\n",
       "      <td>graph3437.png</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4425</th>\n",
       "      <td>graph3649.png</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4426</th>\n",
       "      <td>graph893.png</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4427</th>\n",
       "      <td>graph7058.png</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4428 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_name hamiltonian\n",
       "0     graph4443.png         yes\n",
       "1     graph1905.png          no\n",
       "2     graph7719.png          no\n",
       "3     graph4902.png          no\n",
       "4     graph4114.png         yes\n",
       "...             ...         ...\n",
       "4423  graph4672.png         yes\n",
       "4424  graph3437.png         yes\n",
       "4425  graph3649.png          no\n",
       "4426   graph893.png         yes\n",
       "4427  graph7058.png          no\n",
       "\n",
       "[4428 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.read_csv(\"../data/train_info.csv\")\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    img = np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "    return img / 255\n",
    "\n",
    "def convert_to_array(data_dir, labels):\n",
    "    files = os.listdir(data_dir)\n",
    "    dataset_x = []\n",
    "    dataset_y = []\n",
    "    for picture in files:\n",
    "        img = cv2.imread(data_dir + f\"/{picture}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (100, 100))\n",
    "        img = rgb2gray(img)\n",
    "        \n",
    "        img = img.reshape(1, 100, 100)\n",
    "        \n",
    "        label = labels[labels[\"file_name\"] == f'{picture}'][\"hamiltonian\"].to_list()\n",
    "        \n",
    "        dataset_x.append(img)\n",
    "        dataset_y.append(label[0])\n",
    "    \n",
    "    return np.array(dataset_x), dataset_y\n",
    "\n",
    "data_train = convert_to_array(\"../data/train_images/train_images\", labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train, images_test, labels_train, labels_test = train_test_split(images, labels, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3542, 1, 100, 100), (886, 1, 100, 100))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train.shape, images_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_sample(images, label):\n",
    "    random_indices = np.random.choice(images.shape[0], 12, replace=False)\n",
    "    plt.figure(figsize=(20, 10), dpi=200)\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(images[idx], cmap='gray')\n",
    "        plt.title(f\"Hamiltonian = {label.iloc[idx]['hamiltonian']}\")\n",
    "        plt.axis('on')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.tight_layout()\n",
    "\n",
    "# plot_random_sample(images_train, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(images, labels, num_augmentations):\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for i in range(images.shape[0]):\n",
    "        for _ in range(num_augmentations):\n",
    "            angle = np.random.uniform(-90, 90)\n",
    "            rotated_image = ndimage.rotate(images[i], angle, reshape=True)\n",
    "\n",
    "            shift = np.random.uniform(-5, 5, 2)\n",
    "            shifted_image = ndimage.shift(rotated_image, shift)\n",
    "\n",
    "            if np.random.rand() > 0.5:\n",
    "                flipped_image = np.fliplr(shifted_image)\n",
    "            else:\n",
    "                flipped_image = shifted_image\n",
    "\n",
    "            augmented_images.append(flipped_image)\n",
    "            augmented_labels.append(labels.iloc[i])\n",
    "\n",
    "    max_shape = np.max([img.shape for img in augmented_images], axis=0)\n",
    "\n",
    "    resized_images = []\n",
    "    for img in augmented_images:\n",
    "        resized_img = cv2.resize(img, (max_shape[1], max_shape[0]), interpolation=cv2.INTER_AREA)\n",
    "        resized_images.append(resized_img)\n",
    "\n",
    "    return np.array(resized_images), pd.DataFrame(augmented_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented_data = augment_data(images, label[\"hamiltonian\"], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Moedel Design**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "    \n",
    "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = [[1.0] if i == \"yes\" else [0.0] for i in labels_train]\n",
    "labels_test = [[1.0] if i == \"yes\" else [0.0] for i in labels_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3542, 1, 100, 100]),\n",
       " torch.Size([3542, 1]),\n",
       " torch.Size([886, 1, 100, 100]),\n",
       " torch.Size([886, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_images_train = torch.from_numpy(images_train).type(torch.float).to(device)\n",
    "torch_labels_train = torch.from_numpy(np.array(labels_train)).type(torch.float).to(device)\n",
    "\n",
    "torch_images_test = torch.from_numpy(images_test).type(torch.float).to(device)\n",
    "torch_labels_test = torch.from_numpy(np.array(labels_test)).type(torch.float).to(device)\n",
    "\n",
    "torch_images_train.size(), torch_labels_train.size(), torch_images_test.size(), torch_labels_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset(torch_images_train, torch_labels_train)\n",
    "dataset_test = TensorDataset(torch_images_test, torch_labels_test)\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, \n",
    "    batch_size=32,\n",
    "    # shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    dataset_test, \n",
    "    batch_size=32,\n",
    "    # shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(dataloader_train):\n",
    "    if i == 0:\n",
    "        res = sample\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 100, 100]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].size(), res[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=64,\n",
    "            kernel_size=6,\n",
    "            stride=1,\n",
    "            padding=\"same\"\n",
    "        ) \n",
    "        self.batch_norm_1 = nn.BatchNorm2d(\n",
    "            num_features=64\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=64, \n",
    "            out_channels=128,\n",
    "            kernel_size=16, \n",
    "            stride=1,\n",
    "            padding=\"same\"\n",
    "        )\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(\n",
    "            num_features=128\n",
    "        )\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout2d(p=0.2)\n",
    "        self.dropout_2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=80000, \n",
    "            out_features=128\n",
    "        )\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=128, \n",
    "            out_features=1\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(x.size())\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batch_norm_1(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.dropout_1(x)\n",
    "        \n",
    "        # print(x.size())\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout_1(x)\n",
    "        \n",
    "        x = F.\n",
    "        \n",
    "        # print(x.size())\n",
    "        \n",
    "        x = x.view(-1, 80000)\n",
    "        # print(x.size())\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout_2(x)\n",
    "        \n",
    "        # print(x.size())\n",
    "        x = self.fc2(x)\n",
    "        output = F.sigmoid(x)\n",
    "        # print(output.size())\n",
    "        # x = self.dropout_2(x)\n",
    "        # print(output.size())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(6, 6), stride=(1, 1), padding=same)\n",
       "  (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(16, 16), stride=(1, 1), padding=same)\n",
       "  (batch_norm_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout_1): Dropout2d(p=0.2, inplace=False)\n",
       "  (dropout_2): Dropout(p=0.2, inplace=False)\n",
       "  (fc1): Linear(in_features=80000, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001\n",
    ")\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(\n",
    "    valid_loader: DataLoader,\n",
    "    loss_function: nn.modules.loss._Loss\n",
    ") -> float:\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data_valid, batch_target_valid in dataloader_test:\n",
    "            output_valid = model(batch_data_valid)\n",
    "            loss_valid = loss_function(output_valid, batch_target_valid)\n",
    "            valid_loss += loss_valid.item() * batch_data_valid.size(0)\n",
    "            \n",
    "            predicted = torch.where(output_valid < 0.5, 0.0, 1.0)\n",
    "            total_correct += (predicted == batch_target_valid).sum().item()\n",
    "            \n",
    "    valid_accuracy = 100 * total_correct / len(dataloader_test.dataset)\n",
    "    valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "    return valid_loss, valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] \tTraining Loss: 4.188256 \tValidation Loss: 0.585431 \tTraining Accuracy: 70.4404291360813, \tValidation Accuracy: 68.51015801354401\n",
      "Validation loss decreased (inf --> 0.585431).  Saving model ...\n",
      "Epoch [2/50] \tTraining Loss: 0.527493 \tValidation Loss: 0.932218 \tTraining Accuracy: 76.84923771880294, \tValidation Accuracy: 59.706546275395034\n",
      "Epoch [3/50] \tTraining Loss: 0.442527 \tValidation Loss: 1.250341 \tTraining Accuracy: 79.22077922077922, \tValidation Accuracy: 51.580135440180584\n",
      "Epoch [4/50] \tTraining Loss: 0.400262 \tValidation Loss: 0.358286 \tTraining Accuracy: 82.10050818746471, \tValidation Accuracy: 83.52144469525959\n",
      "Validation loss decreased (0.585431 --> 0.358286).  Saving model ...\n",
      "Epoch [5/50] \tTraining Loss: 0.375882 \tValidation Loss: 1.209999 \tTraining Accuracy: 83.37097684923772, \tValidation Accuracy: 53.160270880361175\n",
      "Epoch [6/50] \tTraining Loss: 0.356054 \tValidation Loss: 1.962069 \tTraining Accuracy: 85.62958780350084, \tValidation Accuracy: 50.33860045146727\n",
      "Epoch [7/50] \tTraining Loss: 0.331851 \tValidation Loss: 1.310944 \tTraining Accuracy: 85.91191417278374, \tValidation Accuracy: 54.17607223476298\n",
      "Epoch [8/50] \tTraining Loss: 0.313235 \tValidation Loss: 1.650508 \tTraining Accuracy: 86.73066064370413, \tValidation Accuracy: 50.90293453724605\n",
      "Epoch [9/50] \tTraining Loss: 0.303595 \tValidation Loss: 1.860915 \tTraining Accuracy: 87.26708074534162, \tValidation Accuracy: 71.55756207674943\n",
      "Epoch [10/50] \tTraining Loss: 0.336052 \tValidation Loss: 2.003204 \tTraining Accuracy: 87.69057029926596, \tValidation Accuracy: 76.2979683972912\n",
      "Epoch [11/50] \tTraining Loss: 0.274471 \tValidation Loss: 0.541378 \tTraining Accuracy: 89.29983060417842, \tValidation Accuracy: 74.94356659142213\n",
      "Epoch [12/50] \tTraining Loss: 0.262705 \tValidation Loss: 4.556989 \tTraining Accuracy: 89.32806324110672, \tValidation Accuracy: 68.2844243792325\n",
      "Epoch [13/50] \tTraining Loss: 0.227757 \tValidation Loss: 1.877446 \tTraining Accuracy: 90.57029926595143, \tValidation Accuracy: 53.95033860045147\n",
      "Epoch [14/50] \tTraining Loss: 0.199831 \tValidation Loss: 2.212696 \tTraining Accuracy: 91.64313946922643, \tValidation Accuracy: 51.128668171557564\n",
      "Epoch [15/50] \tTraining Loss: 0.173503 \tValidation Loss: 1.595738 \tTraining Accuracy: 92.82891022021457, \tValidation Accuracy: 56.54627539503386\n",
      "Epoch [16/50] \tTraining Loss: 0.180604 \tValidation Loss: 1.200283 \tTraining Accuracy: 92.4901185770751, \tValidation Accuracy: 66.93002257336343\n",
      "Epoch [17/50] \tTraining Loss: 0.171853 \tValidation Loss: 3.284704 \tTraining Accuracy: 92.80067758328627, \tValidation Accuracy: 52.25733634311512\n",
      "Epoch [18/50] \tTraining Loss: 0.155203 \tValidation Loss: 4.183044 \tTraining Accuracy: 93.64765669113496, \tValidation Accuracy: 50.79006772009029\n",
      "Epoch [19/50] \tTraining Loss: 0.135825 \tValidation Loss: 8.968206 \tTraining Accuracy: 94.69226425748165, \tValidation Accuracy: 69.41309255079007\n",
      "Epoch [20/50] \tTraining Loss: 0.145294 \tValidation Loss: 1.039839 \tTraining Accuracy: 93.73235460191982, \tValidation Accuracy: 81.15124153498871\n",
      "Epoch [21/50] \tTraining Loss: 0.112778 \tValidation Loss: 0.579237 \tTraining Accuracy: 94.94635798983624, \tValidation Accuracy: 85.77878103837472\n",
      "Epoch [22/50] \tTraining Loss: 0.099505 \tValidation Loss: 13.346061 \tTraining Accuracy: 95.8498023715415, \tValidation Accuracy: 50.45146726862303\n",
      "Epoch [23/50] \tTraining Loss: 0.143523 \tValidation Loss: 1.201564 \tTraining Accuracy: 95.65217391304348, \tValidation Accuracy: 72.686230248307\n",
      "Epoch [24/50] \tTraining Loss: 0.110072 \tValidation Loss: 5.030881 \tTraining Accuracy: 95.70863918690006, \tValidation Accuracy: 50.45146726862303\n",
      "Epoch [25/50] \tTraining Loss: 0.094214 \tValidation Loss: 1.944855 \tTraining Accuracy: 96.47092038396386, \tValidation Accuracy: 55.30474040632054\n",
      "Epoch [26/50] \tTraining Loss: 0.098694 \tValidation Loss: 7.799355 \tTraining Accuracy: 95.68040654997176, \tValidation Accuracy: 52.37020316027088\n",
      "Epoch [27/50] \tTraining Loss: 0.083233 \tValidation Loss: 4.643925 \tTraining Accuracy: 96.8661773009599, \tValidation Accuracy: 57.449209932279906\n",
      "Epoch [28/50] \tTraining Loss: 0.082054 \tValidation Loss: 30.579067 \tTraining Accuracy: 96.61208356860531, \tValidation Accuracy: 58.35214446952596\n",
      "Epoch [29/50] \tTraining Loss: 0.075221 \tValidation Loss: 46.014130 \tTraining Accuracy: 96.8661773009599, \tValidation Accuracy: 50.33860045146727\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 25\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m batch_data_train\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     27\u001b[0m     total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m batch_target_train)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     29\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m total_correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader_train\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_min = np.Inf\n",
    "valid_loss_min = np.Inf\n",
    "last_valid_loss = 0\n",
    "freq = 1\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for batch_data_train, batch_target_train in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_data_train)\n",
    "\n",
    "        predicted = torch.where(output < 0.5, 0.0, 1.0)\n",
    "        \n",
    "        # print(predicted)\n",
    "        \n",
    "        loss = loss_fn(output, batch_target_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss += loss.item() * batch_data_train.size(0)\n",
    "        \n",
    "        total_correct += (predicted == batch_target_train).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * total_correct / len(dataloader_train.dataset)\n",
    "    train_loss = train_loss / len(dataloader_train.dataset)\n",
    "    valid_loss, valid_accuracy = _evaluate(dataloader_test, loss_fn)\n",
    "        \n",
    "    if valid_loss == last_valid_loss:\n",
    "        print('problem')\n",
    "    \n",
    "    last_valid_loss = valid_loss\n",
    "    if epoch % freq == 0:\n",
    "        print(f'Epoch [{epoch}/{n_epochs}] \\tTraining Loss: {train_loss:.6f} \\tValidation Loss: {valid_loss:.6f} \\tTraining Accuracy: {train_accuracy}, \\tValidation Accuracy: {valid_accuracy}')\n",
    "        \n",
    "        \n",
    "    if valid_loss < valid_loss_min:\n",
    "        print(f'Validation loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}).  Saving model ...')\n",
    "        state_dict = model.state_dict()\n",
    "        valid_loss_min = valid_loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
